{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1167c5bf",
   "metadata": {
    "id": "1167c5bf"
   },
   "source": [
    "# 1. What are Ensemble Methods in Machine Learning? What Are They and Why Use Them?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "87ed6f9a",
   "metadata": {
    "id": "87ed6f9a"
   },
   "source": [
    "1. Ensemble is a machine learning technique that combine several base models\n",
    "   in order to produce one optimal predictive model.\n",
    "   M1 - Base model 1\n",
    "   M2 - Base model 2\n",
    "   M3 - Base model 3\n",
    "   M4 - Base model 4\n",
    "   optimal predictive model - Final model\n",
    "\n",
    "2. Ensemble is one of the methods to avoid overfitting in Decision trees.\n",
    "\n",
    "3. Ensemble methods:\n",
    "   1. Bagging - Random forest algorithm uses bagging technique.\n",
    "   2. Boosting - Adaboost algorithm uses boosting technique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f858f19f",
   "metadata": {
    "id": "f858f19f"
   },
   "source": [
    "# 2. Explain the difference between bagging and boosting."
   ]
  },
  {
   "cell_type": "raw",
   "id": "d7ba3636",
   "metadata": {
    "id": "d7ba3636"
   },
   "source": [
    "Bagging:\n",
    "    1. Bagging decreases variance and not bias.\n",
    "    2. It follows parallel approach.\n",
    "    3. eg - Random forest uses bagging technique.\n",
    "    4. Each model receives equal weight.\n",
    "    5. Reduces overfitting problem.\n",
    "\n",
    "Boosting:\n",
    "    1. Boosting decreases bias and not variance.\n",
    "    2. It follows sequential approach.\n",
    "    3. eg - Adaboost uses boosting technique.\n",
    "    4. Model are weighed according to their performance.\n",
    "    5. can increase overfitting problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac2f7a3",
   "metadata": {
    "id": "1ac2f7a3"
   },
   "source": [
    "# 3. Why is Random Forest Algorithm popular?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e635df1e",
   "metadata": {
    "id": "e635df1e"
   },
   "source": [
    "1. Can be used for both classification as well as regression but mainly used\n",
    "   for classification.\n",
    "2. Very few assumptions therefore data preparing is less challenging and time\n",
    "   saving.\n",
    "3. Less training time compared to other algorithms.\n",
    "4. Predicts output with high accuracy even for large dataset.\n",
    "5. Accuracy is maintained even when large proportion of data is missing.\n",
    "6. Prevent overfitting.\n",
    "7. Works well on non-linear data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e40399",
   "metadata": {
    "id": "f0e40399"
   },
   "source": [
    "# 4. Can Random Forest Algorithm be used both for Continuous and Categorical Target Variables?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0af56c33",
   "metadata": {
    "id": "0af56c33"
   },
   "source": [
    "Yes, Random Forest can be used for both continuous and categorical target (dependent) variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57aa764",
   "metadata": {
    "id": "d57aa764"
   },
   "source": [
    "# 5. What do you mean by Bagging?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "382d0a16",
   "metadata": {
    "id": "382d0a16"
   },
   "source": [
    "1. Bagging means bootstrapp + Aggregating.\n",
    "\n",
    "2. Bagging uses the parallel approach(parallel training of models/models\n",
    "   are trained individually).\n",
    "\n",
    "3. From 1 dataset we have to create multiple datasets which will be\n",
    "   called bootstrap dataset.This bootstrap datasets will be unique.\n",
    "\n",
    "4. Then after creating bootstrap dataset we have to train the model(M1,M2,M3)\n",
    "   and combine the output of 3 models and send it to VC(Voting classifier).\n",
    "\n",
    "5. For classifition - predicted class will be class having maximum votes.\n",
    "   For Regression - Just find the mean of the output of the decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1855e1f",
   "metadata": {
    "id": "a1855e1f"
   },
   "source": [
    "# 6. Explain the working of the Random Forest Algorithm."
   ]
  },
  {
   "cell_type": "raw",
   "id": "f02c4b02",
   "metadata": {
    "id": "f02c4b02"
   },
   "source": [
    "1. It is a supervised machine learning algorithm used for both classification\n",
    "   and regression.\n",
    "\n",
    "2. It is based on ensemble technique that combine several base models in order\n",
    "   to produce one optimal predictive model. Random forest uses the bagging\n",
    "   ensemble method.\n",
    "\n",
    "3. Instead of relying on one decision tree, the random forest takes the\n",
    "   prediction from each tree and based on the majority votes of\n",
    "   predictions, and it predicts the final output.\n",
    "\n",
    "4. The greater number of trees in the forest leads to higher accuracy and\n",
    "   prevents the problem of overfitting.\n",
    "\n",
    "working:\n",
    "1. Assume “m” features in our dataset.\n",
    "\n",
    "2. Randomly chose “k” features satisfying condition k < m.Build and train a\n",
    "   decision tree model on these K features.\n",
    "\n",
    "3. Among the k features, calculate the root node by choosing a node with the\n",
    "   highest Information gain.\n",
    "\n",
    "4. Split the node into child nodes.\n",
    "\n",
    "5. choose the number of trees and Repeat the same procedure for all the trees.\n",
    "\n",
    "7. Perform bagging, i.e., combining the results of all Decision Trees.\n",
    "\n",
    "For classification problem - predicted class will be that class having the maximum number of votes.\n",
    "For Regression problem - Final output will be mean of all decision tree outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4caa392",
   "metadata": {
    "id": "d4caa392"
   },
   "source": [
    "# 7. Why do we prefer a Forest (collection of Trees) rather than a single Tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c10c227",
   "metadata": {
    "id": "0c10c227"
   },
   "outputs": [],
   "source": [
    " Because it gives less variance.It reduces overfitting.Therefore more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a455973",
   "metadata": {
    "id": "5a455973"
   },
   "source": [
    "# 8. What do you mean by Bootstrap Sample?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c826a500",
   "metadata": {
    "id": "c826a500"
   },
   "source": [
    "1. In this process, we are picking the samples randomly from the main dataset\n",
    "   with replacement.\n",
    "\n",
    "2. Suppose there are 1000 samples in main dataset then in bootstrapp dataset\n",
    "   also there will be 1000 rows but this 1000 rows are not same in both.\n",
    "\n",
    "3. While creating a bootstrapp dataset, samples are picked randomly from main\n",
    "   dataset.\n",
    "\n",
    "4. In boostrap dataset, there are some duplicate samples as well(All samples\n",
    "   should not be unique).\n",
    "\n",
    "5. suppose we are creating 100 bootstrap datasets, then this 100 bootstrap\n",
    "   datasets will be unique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73fbd68",
   "metadata": {
    "id": "b73fbd68"
   },
   "source": [
    "# 9. What does random refer to in ‘Random Forest’?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b9496006",
   "metadata": {
    "id": "b9496006"
   },
   "source": [
    "1. While creating a bootstrapp dataset,  we are picking the samples randomly\n",
    "   from the main dataset with replacement.\n",
    "\n",
    "2. we are selecting some random features from the total number of features and\n",
    "   out of this random features we are finding the best feature that will come\n",
    "   at the decision node.\n",
    "\n",
    "   eg. if there are 25 total number of features then, then default value of\n",
    "       max_features = sqrt(no of features)=5 which means out of 25 features\n",
    "       model will pick 5 random features and out of 5 features we will find\n",
    "       our best feature which will be our decision node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290eb15c",
   "metadata": {
    "id": "290eb15c"
   },
   "source": [
    "# 10. List down the features of Bagged Trees."
   ]
  },
  {
   "cell_type": "raw",
   "id": "9289f224",
   "metadata": {
    "id": "9289f224"
   },
   "source": [
    "1. Bagged trees reduces variance.\n",
    "2. Uses entire feature space while splitting.\n",
    "3. Tree grows without pruning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c797a58f",
   "metadata": {
    "id": "c797a58f"
   },
   "source": [
    "# 11. What are the Limitations of Bagging Trees?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c90ff5f8",
   "metadata": {
    "id": "c90ff5f8"
   },
   "source": [
    "Bagging tree Uses entire feature space while splitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda1387a",
   "metadata": {
    "id": "bda1387a"
   },
   "source": [
    "# 12. Explain how the Random Forests give output for Classification, and Regression problems?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a761bacd",
   "metadata": {
    "id": "a761bacd"
   },
   "source": [
    "For classification problem - predicted class will be that class having the\n",
    "                             maximum number of votes.\n",
    "For Regression problem - Final output will be mean of all decision tree\n",
    "                         outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3f66e8",
   "metadata": {
    "id": "1f3f66e8"
   },
   "source": [
    "#  13. Does Random Forest need Pruning? Why or why not?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "883badb1",
   "metadata": {
    "id": "883badb1"
   },
   "source": [
    "1. Pruning is required in decision trees to avoid overfitting.\n",
    "\n",
    "2. In random forest, the data sample going to each individual tree has\n",
    "   already gone through bagging(which is again responsible for dealing\n",
    "   with overfitting).\n",
    "\n",
    "3. There is no need to go for Pruning in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783de5d2",
   "metadata": {
    "id": "783de5d2"
   },
   "source": [
    "# 14. List down the hyper-parameters used to fine-tune the Random Forest."
   ]
  },
  {
   "cell_type": "raw",
   "id": "61e65664",
   "metadata": {
    "id": "61e65664"
   },
   "source": [
    "1. criterion : {\"gini\", \"entropy\"}, default=\"gini\"\n",
    "   The function to measure the quality of a split.\n",
    "\n",
    "   criterion : {\"mse\", \"mae\"}, default=\"mse\"\n",
    "   The function to measure the quality of a split.\n",
    "\n",
    "2. max_depth : int, default=None\n",
    "   The maximum depth of the tree. If None, then nodes are expanded until\n",
    "   all leaves are pure.\n",
    "\n",
    "3. min_samples_split : int or float, default=2\n",
    "   The minimum number of samples required to split an internal node\n",
    "\n",
    "4. min_samples_leaf : int or float, default=1\n",
    "   The minimum number of samples required to be at a leaf node.\n",
    "\n",
    "5. n_estimators : int, default=100\n",
    "   The number of trees in the forest.\n",
    "\n",
    "6. max_features : {\"auto\", \"sqrt\", \"log2\"}, int or float, default=\"auto\"\n",
    "   The number of features to consider when looking for the best split:\n",
    "\n",
    "    - If int, then consider `max_features` features at each split.\n",
    "    - If float, then `max_features` is a fraction and\n",
    "      `round(max_features * n_features)` features are considered at each\n",
    "      split.\n",
    "    - If \"auto\", then `max_features=n_features`.\n",
    "    - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
    "    - If \"log2\", then `max_features=log2(n_features)`.\n",
    "    - If None, then `max_features=n_features`.\n",
    "\n",
    "7. bootstrap : bool, default=True\n",
    "   Whether bootstrap samples are used when building trees. If False, the\n",
    "   whole dataset is used to build each tree.\n",
    "\n",
    "8. oob_score : bool, default=False\n",
    "    Whether to use out-of-bag samples to estimate the generalization score.\n",
    "    Only available if bootstrap=True."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7cd7ca",
   "metadata": {
    "id": "7c7cd7ca"
   },
   "source": [
    "## RandomForestClassifier hyperparameters"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a70bbddd",
   "metadata": {
    "id": "a70bbddd"
   },
   "source": [
    "1. criterion='gini',\n",
    "2. max_depth=None,\n",
    "3. min_samples_split=2,\n",
    "4. min_samples_leaf=1,\n",
    "5. n_estimators=100,\n",
    "6. max_features='auto',\n",
    "7. bootstrap=True,\n",
    "8. oob_score=False,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5341cd",
   "metadata": {
    "id": "0c5341cd"
   },
   "source": [
    "## RandomForestRegressor hyperparameters"
   ]
  },
  {
   "cell_type": "raw",
   "id": "747d6d90",
   "metadata": {
    "id": "747d6d90"
   },
   "source": [
    "1. criterion='mse',\n",
    "2. max_depth=None,\n",
    "3. min_samples_split=2,\n",
    "4. min_samples_leaf=1,\n",
    "5. n_estimators=100,\n",
    "6. max_features='auto',\n",
    "7. bootstrap=True,\n",
    "8. oob_score=False,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2119cab4",
   "metadata": {
    "id": "2119cab4"
   },
   "source": [
    "# 15. What is the importance of max_feature hyperparameter?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1b93a7f3",
   "metadata": {
    "id": "1b93a7f3"
   },
   "source": [
    "1. In decision tree, we are finding the best feature from all the features.\n",
    "   ie. we are checking GI and Information gain of all the features.\n",
    "\n",
    "2. In random forest, suppose there are 1000 decision trees so if we go on\n",
    "   checking entropy and GI for all the features then time complexity will\n",
    "   increase.In this case max_features parameter will come into picture.\n",
    "\n",
    "3. max_features : {\"auto\", \"sqrt\", \"log2\"}, int or float, default=\"auto\"\n",
    "   The number of features to consider when looking for the best split:\n",
    "\n",
    "    - If int, then consider `max_features` features at each split.\n",
    "    - If float, then `max_features` is a fraction and\n",
    "      `round(max_features * n_features)` features are considered at each\n",
    "      split.\n",
    "    - If \"auto\", then `max_features=n_features`.\n",
    "    - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
    "    - If \"log2\", then `max_features=log2(n_features)`.\n",
    "    - If None, then `max_features=n_features`.\n",
    "\n",
    "4. eg if there are 25 total number of features then,then default value\n",
    "   of max_features = sqrt(no of features)=5 which means out of 25\n",
    "   features model will pick 5 random features and out of 5 features we\n",
    "   will find our best feature which will be our decision node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61268d6",
   "metadata": {
    "id": "f61268d6"
   },
   "source": [
    "# 16. What are the advantages and disadvantages of the Random Forest Algorithm?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "964ceff4",
   "metadata": {
    "id": "964ceff4"
   },
   "source": [
    "Advantages:\n",
    "    1. It can be used for classification as well as regression.\n",
    "    2. It is non-parametric algorithm ie no assumption on features.\n",
    "    3. works well for catergorical as well as continuous data.\n",
    "    4. Feature scaling is not required.\n",
    "    5. perform well on non-linear data.\n",
    "    6. Not impacted by outliers.\n",
    "    7. Predicts output with high accuracy even for large dataset.\n",
    "    8. Accuracy is maintained even when large proportion of data is missing.\n",
    "    9. More stable than decision tree.\n",
    "\n",
    "Disadvantages:\n",
    "    1. Time complexity is high because we use multiple decision tree(very\n",
    "       complex algorithm).\n",
    "    2. Longer training time required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f9911c",
   "metadata": {
    "id": "43f9911c"
   },
   "source": [
    "# 17. What are the applications are random forests?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bc594498",
   "metadata": {
    "id": "bc594498"
   },
   "source": [
    "1. credit card fraud detection.\n",
    "2. Stock market price prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9432076d",
   "metadata": {
    "id": "9432076d"
   },
   "source": [
    "# 18. What are the Out-of-Bag samples?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4aeb6c02",
   "metadata": {
    "id": "4aeb6c02"
   },
   "source": [
    "Samples not used while training of bootstrap dataset.\n",
    "\n",
    "eg.\n",
    "Main dataset   BD1     BD2\n",
    "     1          2       1\n",
    "     2          1       2\n",
    "     3          5       3\n",
    "     4          2       4\n",
    "     5          1       4\n",
    "     6          3       6\n",
    "\n",
    "In BD1, Out-of-Bag samples is 6 because 6 is not used in training of BD1.\n",
    "We have build this DT by training on just 5 rows.\n",
    "\n",
    "In BD2, Out-of-Bag samples is 5 because 5 is not used in training of BD2.\n",
    "We have build this DT by training on just 5 rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a077093",
   "metadata": {
    "id": "4a077093"
   },
   "source": [
    "# 19. What is Out-of-Bag Error?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "96d024d0",
   "metadata": {
    "id": "96d024d0"
   },
   "source": [
    "1. Out-of Bag Error  =\n",
    "   Number of incorrectly classified OOB samples by the classifier\n",
    "  ---------------------------------------------------------------\n",
    "                         Total OOB samples\n",
    "\n",
    "2. This should be as low as possible for better performance of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88c62ad",
   "metadata": {
    "id": "d88c62ad"
   },
   "source": [
    "# 20. How would you improve the performance of Random Forest?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4ca2e5d3",
   "metadata": {
    "id": "4ca2e5d3"
   },
   "source": [
    "1. Specify the maximum depth of the trees. By default, trees are expanded\n",
    "   until all leaves are either pure or contain less than the minimum samples\n",
    "   for the split. This can still cause the trees to overfit or underfit.\n",
    "   Play with the hyperparameter to find an optimal number for max_depth.\n",
    "\n",
    "2. Increase or decrease the number of estimators.\n",
    "   RF accuracy increase - increase Number of trees\n",
    "   RF speed increase - decrease Number of trees\n",
    "\n",
    "3. Specify the maximum number of features to be included at each node split.\n",
    "   This depends very heavily on your dataset.\n",
    "   IV are highly correlated - decrease the maximum number of features.\n",
    "   If your input attributes are not correlated and your model is suffering\n",
    "   from low accuracy, increase the number of features to be included."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
